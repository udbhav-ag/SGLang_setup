######## SGLang Config ###########

# Model and Tokenizer
model: meta-llama/Llama-3.2-3B
# tokenizer-path:
tokenizer-mode: auto
trust-remote-code: true
chat-template: llama-2
context-length: 64000 # defaults to the model's config.json value


# HTTP Server
host: 0.0.0.0
port: 30000

# Quantization and dtye
dtype: auto
# quantization: none
# quantization-param-path: 
kv-cache-dtype: auto

allow-auto-truncate: false
# Memory and Scheduling
mem-fraction-static: 0.2  # Fraction of GPU memory to preallocate at startup for its static memory pool, primarily the KV cache  after loading model weights (0.0 - 1.0)
# max-running-requests: 16  # Maximum number of concurrent requests the server can handle
# max-queued-requests: 64  # Maximum number of requests that can be queued when all running request slots are full
max-total-tokens: 64000  # the absolute maximum tokens that can be held in the KV cache memory pool across all concurrent requests combined. I
chunked-prefill-size: -1 # Maximum number of tokens to process in a single chunk during the prefill phase. -1 means no chunking.
schedule-policy: fcfs # Scheduling policy for handling requests. Options: fcfs (default), lpm, random, dfs-weights, lof, priority, routing-key
enable-priority-scheduling: false # Enable priority-based scheduling for requests.
abort-on-priority-when-disabled: false # Abort requests with priority when priority scheduling is disabled.
schedule-low-priority-values-first: false # Schedule requests with lower priority values first when priority scheduling is enabled.
priority-scheduling-preemption-threshold: 10 # Threshold for preempting running requests based on priority.
radix-eviction-policy: lru # Eviction policy for radix cache. Options: lru (default), lfu


# Runtime Options
device : cuda  # Options: cuda, cpu, xpu,hpu, npu
tensor-parallel-size: 1  # Number of devices to use for tensor parallelism
# pipeline-parallel-size: 1  # Number of devices to use for pipeline parallelism
download-dir: ./models

# Logging
log-level: debug  # Options: debug, info, warning, error, critical
log-level-http: debug
log-requests: true
log-requests-level: 3
# log-requests-format: text
enable-metrics: true



# Metrics Export
export-metrics-to-file: true
export-metrics-to-file-dir: ./metrics
enable-cache-report: true


# Kernel Backend 
## Need to review these

# HiCache
enable-hierarchical-cache: true
hicache-ratio: 1  # Ratio of size of host KV cache memory pool to the size of the device pool
hicache-write-policy: write_through  # Options: write-back, write-through, write-through-selective
# How data moves between GPU - CPU.
# 1. write_through (default) Every KV written to GPU is also immediately copied to CPU.
# 2. write_back Only write to CPU when GPU is full.
# 3. write_through_selective Smart version: only important KV written to CPU.

hicache-io-backend: kernel # how data is physically moved between GPU and CPU for nvidia-gpu
hicache-mem-layout:  layer_first # how CPU KV memory is organized.
hicache-storage-backend: mooncake # READ THE DOCS FOR THIS
hicache-storage-prefetch-policy: best_effort # When KV is on storage backend, and a request needs it, should SGLang best_effort, wait_complete, timeout
hicache-storage-backend-extra-config: '{"master_server_address": "18.232.108.218:50051",
  "local_hostname": "0.0.0.0",
  "metadata_server": "http://18.232.108.218:8080/metadata",
  "global_segment_size": "0",
  "local_buffer_size": "4gb",
  "protocol": "tcp",
  "device_name": "",
  "transfer_timeout": 2,
  "save_chunk_meta": true
}'
# extra config for storage backend, refer to docs for details

# Mooncake Specific Configs

# PD Dissag Specific Configs

