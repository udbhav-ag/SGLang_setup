model: meta-llama/Llama-3.2-1B
# tokenizer-path:
tokenizer-mode: auto
trust-remote-code: true
chat-template: llama-2
device : cpu  # Options: cuda, cpu, xpu,hpu, npu
attention-backend: torch_native
kv-cache-dtype: bfloat16

# CPU OOM guardrails: cap preallocated KV pool instead of auto-sizing from host RAM.
# mem-fraction-static: 0.6
max-total-tokens: 120000
# max-running-requests: 8
# max-prefill-tokens: 2048
chunked-prefill-size: -1

run-name: decode-18-feb
batch-size: 8
input-len: 2048
output-len: 128
prompt-filename: /Udbhav/SGLang_setup/prompts/1.txt
correctness-test: true
